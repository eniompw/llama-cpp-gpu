# LLaMA.cpp GPU

Offloads some of the model layers to the GPU, allowing larger models to be loaded

* [cuBLAS](https://github.com/ggerganov/llama.cpp#cublas)
* [Model](https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML#how-to-run-in-llamacpp)
