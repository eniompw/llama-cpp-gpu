# LLaMA.cpp GPU

Offloads some of the model layers to the GPU, allowing larger models to be loaded

![colab resources](https://github.com/eniompw/llama-cpp-gpu/blob/main/colab-resources.png)

* [cuBLAS](https://github.com/ggerganov/llama.cpp#cublas)
* [Model](https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML#how-to-run-in-llamacpp)
