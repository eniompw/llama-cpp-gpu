# LLaMA.cpp GPU

Offloads some of the model layers to the GPU, allowing larger models to be loaded

![model](https://github.com/eniompw/llama-cpp-gpu/blob/main/model-details.png)
![colab resources](https://github.com/eniompw/llama-cpp-gpu/blob/main/colab-resources.png)
![parameters](https://github.com/eniompw/llama-cpp-gpu/blob/main/size-parameters.png)

* [cuBLAS](https://github.com/ggerganov/llama.cpp#cublas)
* [Model](https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML#how-to-run-in-llamacpp)
