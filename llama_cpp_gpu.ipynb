{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eniompw/llama-cpp-gpu/blob/main/llama_cpp_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvjhm3p6Qwzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00628992-28ac-4b47-9671-152864a38013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 3946, done.\u001b[K\n",
            "remote: Counting objects: 100% (1712/1712), done.\u001b[K\n",
            "remote: Compressing objects: 100% (270/270), done.\u001b[K\n",
            "remote: Total 3946 (delta 1577), reused 1475 (delta 1442), pack-reused 2234\u001b[K\n",
            "Receiving objects: 100% (3946/3946), 3.40 MiB | 7.32 MiB/s, done.\n",
            "Resolving deltas: 100% (2670/2670), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/ggerganov/llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd llama.cpp && make clean && make LLAMA_CUBLAS=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogEokmNkYxUu",
        "outputId": "61bcfbea-fc7d-494e-8ea4-acff5b826274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server vdot train-text-from-scratch build-info.h\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
            "I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
            "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
            "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_DMMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid write_tensor(llama_file*, ggml_tensor*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:2371:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K-\u001b[m\u001b[K’ in operand of ‘\u001b[01m\u001b[K&\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            " 2371 |         file->seek(\u001b[01;35m\u001b[K0-file->tell()\u001b[m\u001b[K & 31, SEEK_CUR);\n",
            "      |                    \u001b[01;35m\u001b[K~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:2386:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K-\u001b[m\u001b[K’ in operand of ‘\u001b[01m\u001b[K&\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            " 2386 |     file->seek(\u001b[01;35m\u001b[K0-file->tell()\u001b[m\u001b[K & 31, SEEK_CUR);\n",
            "      |                \u001b[01;35m\u001b[K~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid read_tensor(llama_file*, ggml_tensor*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:2407:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K-\u001b[m\u001b[K’ in operand of ‘\u001b[01m\u001b[K&\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            " 2407 |     file->seek(\u001b[01;35m\u001b[K0-file->tell()\u001b[m\u001b[K & 31, SEEK_CUR);\n",
            "      |                \u001b[01;35m\u001b[K~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:495\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/include/c++/9/cstring:42\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:7\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:305:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:306:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:307:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd llama.cpp/models/ && wget https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML/resolve/main/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkPLIZBrdrhI",
        "outputId": "d3476703-de2d-4fe1-93b0-a32467fefb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-23 06:50:19--  https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML/resolve/main/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 65.9.86.71, 65.9.86.62, 65.9.86.57, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.9.86.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/cb/a0/cba0533d6f415078c26493f36d1d844f10f63bdc471979fd7b359381a2eb0361/76bea6b983f79a4dda3228a9d29760103ae0a8d3970eb5cd4239722ea86ed809?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin%3B+filename%3D%22Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687762220&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NiL2EwL2NiYTA1MzNkNmY0MTUwNzhjMjY0OTNmMzZkMWQ4NDRmMTBmNjNiZGM0NzE5NzlmZDdiMzU5MzgxYTJlYjAzNjEvNzZiZWE2Yjk4M2Y3OWE0ZGRhMzIyOGE5ZDI5NzYwMTAzYWUwYThkMzk3MGViNWNkNDIzOTcyMmVhODZlZDgwOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc3NjIyMjB9fX1dfQ__&Signature=RXlidFMzgLmeoEraIcImcMGZMAaVgLKY4ufGI8oD6F9054hwjiZsU6ypusVcBTkJTvGaQ4RVVeWiJhlWGZ%7EodUzGr8NN5vQlWPNRWqCVXETpC4vxbRB0VrsXb3LujqAxDraYxHjiTNAztNEr80uUDqL4Noo6o%7EDS5sT1VMNCMmHYamjgV7CQGOXoLwootUqmZF8%7Eb4cnKTRVg9wDJgqXPtgXOU-vXINjNY%7EZCrmBiqWRz%7EofjSTyh8qcn%7EgqCOeYjvJWB0ajiOGvlmpB6qOk5L6OiP%7EU5DzzYjNp9xPwB7AMdeNAvU%7E4Njl08Xk4nx1oUHndK5niYeChWWh95UnEnw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-23 06:50:20--  https://cdn-lfs.huggingface.co/repos/cb/a0/cba0533d6f415078c26493f36d1d844f10f63bdc471979fd7b359381a2eb0361/76bea6b983f79a4dda3228a9d29760103ae0a8d3970eb5cd4239722ea86ed809?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin%3B+filename%3D%22Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687762220&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NiL2EwL2NiYTA1MzNkNmY0MTUwNzhjMjY0OTNmMzZkMWQ4NDRmMTBmNjNiZGM0NzE5NzlmZDdiMzU5MzgxYTJlYjAzNjEvNzZiZWE2Yjk4M2Y3OWE0ZGRhMzIyOGE5ZDI5NzYwMTAzYWUwYThkMzk3MGViNWNkNDIzOTcyMmVhODZlZDgwOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc3NjIyMjB9fX1dfQ__&Signature=RXlidFMzgLmeoEraIcImcMGZMAaVgLKY4ufGI8oD6F9054hwjiZsU6ypusVcBTkJTvGaQ4RVVeWiJhlWGZ%7EodUzGr8NN5vQlWPNRWqCVXETpC4vxbRB0VrsXb3LujqAxDraYxHjiTNAztNEr80uUDqL4Noo6o%7EDS5sT1VMNCMmHYamjgV7CQGOXoLwootUqmZF8%7Eb4cnKTRVg9wDJgqXPtgXOU-vXINjNY%7EZCrmBiqWRz%7EofjSTyh8qcn%7EgqCOeYjvJWB0ajiOGvlmpB6qOk5L6OiP%7EU5DzzYjNp9xPwB7AMdeNAvU%7E4Njl08Xk4nx1oUHndK5niYeChWWh95UnEnw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.156.60.44, 108.156.60.37, 108.156.60.112, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.156.60.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13831029888 (13G) [application/octet-stream]\n",
            "Saving to: ‘Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin’\n",
            "\n",
            "Wizard-Vicuna-13B-U 100%[===================>]  12.88G   239MB/s    in 55s     \n",
            "\n",
            "2023-06-23 06:51:15 (242 MB/s) - ‘Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin’ saved [13831029888/13831029888]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd llama.cpp && ./main -t 2 -ngl 32 -m ./models/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction: Write a story about llamas\\n### Response:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM_9_PzYYeiw",
        "outputId": "a2e884ad-6c66-44cf-f272-bbbdd5d6051f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 725 (7487137)\n",
            "main: seed  = 1687503157\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4\n",
            "llama.cpp: loading model from ./models/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 2048\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
            "llama_model_load_internal: using CUDA for GPU acceleration\n",
            "llama_model_load_internal: mem required  = 4951.70 MB (+ 1608.00 MB per state)\n",
            "llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\n",
            "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
            "llama_model_load_internal: offloaded 32/43 layers to GPU\n",
            "llama_model_load_internal: total VRAM used: 10799 MB\n",
            "....................................................................................................\n",
            "llama_init_from_file: kv self size  = 1600.00 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m ### Instruction: Write a story about llamas\\n### Response:\u001b[0m Once upon a time, in the Andes Mountains of South America, there was a herd of llamas. They were known for their unique appearance - tall and lean with long necks and soft fur in shades of brown and gray. The llamas lived happily together, grazing on grassy hillsides and taking leisurely walks through the mountainside. \n",
            "One day, a group of tourists came to visit the herd. They were fascinated by these strange creatures with their long necks and fluffy fur. The llamas seemed to enjoy the attention, posing for pictures and allowing themselves to be petted and scratched behind the ears.\n",
            "As the days went on, the llamas became more and more curious about the tourists. They would watch from a distance as people hiked through the mountains or rode horses past their field. The llamas soon realized that they could have fun with these visitors too! \n",
            "One day, the llamas decided to play a trick on the tourists. They ran around in circles, making loud noises and spitting at anyone who tried to get close. The tourists were surprised and amused by this behavior, laughing as they tried to dodge the spit. \n",
            "From that day forward, the llamas would often play tricks on the visitors, surprising them with their silly antics and unique personalities. The llamas became famous throughout the region for their entertaining ways, drawing tourists from all over the world to see these amazing creatures. [end of text]\n",
            "\n",
            "llama_print_timings:        load time = 75008.24 ms\n",
            "llama_print_timings:      sample time =   209.25 ms /   324 runs   (    0.65 ms per token,  1548.38 tokens per second)\n",
            "llama_print_timings: prompt eval time = 13142.48 ms /    17 tokens (  773.09 ms per token,     1.29 tokens per second)\n",
            "llama_print_timings:        eval time = 169278.93 ms /   323 runs   (  524.08 ms per token,     1.91 tokens per second)\n",
            "llama_print_timings:       total time = 182746.61 ms\n"
          ]
        }
      ]
    }
  ]
}