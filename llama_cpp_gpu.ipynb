{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eniompw/llama-cpp-gpu/blob/main/llama_cpp_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yvjhm3p6Qwzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3c6b7b-3550-4685-bc1e-ae29ddc7eb27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 3981, done.\u001b[K\n",
            "remote: Counting objects: 100% (1747/1747), done.\u001b[K\n",
            "remote: Compressing objects: 100% (285/285), done.\u001b[K\n",
            "remote: Total 3981 (delta 1603), reused 1505 (delta 1462), pack-reused 2234\u001b[K\n",
            "Receiving objects: 100% (3981/3981), 3.42 MiB | 20.10 MiB/s, done.\n",
            "Resolving deltas: 100% (2696/2696), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/ggerganov/llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd llama.cpp && make LLAMA_CUBLAS=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogEokmNkYxUu",
        "outputId": "785b90e1-a0bf-4bcb-ba3f-84485831f5d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
            "I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
            "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
            "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_DMMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid write_tensor(llama_file*, ggml_tensor*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:2371:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K-\u001b[m\u001b[K’ in operand of ‘\u001b[01m\u001b[K&\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            " 2371 |         file->seek(\u001b[01;35m\u001b[K0-file->tell()\u001b[m\u001b[K & 31, SEEK_CUR);\n",
            "      |                    \u001b[01;35m\u001b[K~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:2386:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K-\u001b[m\u001b[K’ in operand of ‘\u001b[01m\u001b[K&\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            " 2386 |     file->seek(\u001b[01;35m\u001b[K0-file->tell()\u001b[m\u001b[K & 31, SEEK_CUR);\n",
            "      |                \u001b[01;35m\u001b[K~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid read_tensor(llama_file*, ggml_tensor*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:2407:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K-\u001b[m\u001b[K’ in operand of ‘\u001b[01m\u001b[K&\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            " 2407 |     file->seek(\u001b[01;35m\u001b[K0-file->tell()\u001b[m\u001b[K & 31, SEEK_CUR);\n",
            "      |                \u001b[01;35m\u001b[K~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:495\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/include/c++/9/cstring:42\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:7\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:305:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:306:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:307:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd llama.cpp/models/ && wget https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML/resolve/main/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkPLIZBrdrhI",
        "outputId": "9060c2ee-f13a-4c24-93b8-50b8493d9f7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-23 22:14:18--  https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML/resolve/main/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 99.84.160.64, 99.84.160.57, 99.84.160.43, ...\n",
            "Connecting to huggingface.co (huggingface.co)|99.84.160.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/cb/a0/cba0533d6f415078c26493f36d1d844f10f63bdc471979fd7b359381a2eb0361/76bea6b983f79a4dda3228a9d29760103ae0a8d3970eb5cd4239722ea86ed809?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin%3B+filename%3D%22Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687817659&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NiL2EwL2NiYTA1MzNkNmY0MTUwNzhjMjY0OTNmMzZkMWQ4NDRmMTBmNjNiZGM0NzE5NzlmZDdiMzU5MzgxYTJlYjAzNjEvNzZiZWE2Yjk4M2Y3OWE0ZGRhMzIyOGE5ZDI5NzYwMTAzYWUwYThkMzk3MGViNWNkNDIzOTcyMmVhODZlZDgwOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc4MTc2NTl9fX1dfQ__&Signature=gfEI%7EiqFsqcANdr8GyXjkYROevF41EcAO4RRyS-AkwXhJmmHSQYx6%7EKxuQLllEtKRxjAFplnehIy7KsWL1CHZqI-oeF8Bp9Sj-owh01ifiQdEVaLpK41OEljfLBdaGoNSl5-FHr1lEs2LNttJa%7EVFzttD3hFwKqjqyd6zmimLzIsUFjfp9ws9konX9silbyFgfE7GvjCFgM35AVDiVA5Twf5UFVH8sjKl9Nb2gUSaOyVcdsN%7EZZyIOQk-DnV-nBrpIIpe4A%7E-UP23vOkxQVR8Iwt-W7Bn1gx%7Eif2VNmtaJ3GrPJQbvMYfWQuP3EhCpS7E8t18tfWkc6PciAMQLNj-A__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-23 22:14:18--  https://cdn-lfs.huggingface.co/repos/cb/a0/cba0533d6f415078c26493f36d1d844f10f63bdc471979fd7b359381a2eb0361/76bea6b983f79a4dda3228a9d29760103ae0a8d3970eb5cd4239722ea86ed809?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin%3B+filename%3D%22Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687817659&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NiL2EwL2NiYTA1MzNkNmY0MTUwNzhjMjY0OTNmMzZkMWQ4NDRmMTBmNjNiZGM0NzE5NzlmZDdiMzU5MzgxYTJlYjAzNjEvNzZiZWE2Yjk4M2Y3OWE0ZGRhMzIyOGE5ZDI5NzYwMTAzYWUwYThkMzk3MGViNWNkNDIzOTcyMmVhODZlZDgwOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc4MTc2NTl9fX1dfQ__&Signature=gfEI%7EiqFsqcANdr8GyXjkYROevF41EcAO4RRyS-AkwXhJmmHSQYx6%7EKxuQLllEtKRxjAFplnehIy7KsWL1CHZqI-oeF8Bp9Sj-owh01ifiQdEVaLpK41OEljfLBdaGoNSl5-FHr1lEs2LNttJa%7EVFzttD3hFwKqjqyd6zmimLzIsUFjfp9ws9konX9silbyFgfE7GvjCFgM35AVDiVA5Twf5UFVH8sjKl9Nb2gUSaOyVcdsN%7EZZyIOQk-DnV-nBrpIIpe4A%7E-UP23vOkxQVR8Iwt-W7Bn1gx%7Eif2VNmtaJ3GrPJQbvMYfWQuP3EhCpS7E8t18tfWkc6PciAMQLNj-A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.249.85.12, 13.249.85.11, 13.249.85.23, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.249.85.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13831029888 (13G) [application/octet-stream]\n",
            "Saving to: ‘Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin’\n",
            "\n",
            "Wizard-Vicuna-13B-U 100%[===================>]  12.88G  54.5MB/s    in 3m 33s  \n",
            "\n",
            "2023-06-23 22:17:52 (62.0 MB/s) - ‘Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin’ saved [13831029888/13831029888]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd llama.cpp && ./main -t 2 -ngl 32 -m ./models/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction: Write a story about llamas\\n### Response:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM_9_PzYYeiw",
        "outputId": "f892c912-bbd9-4c3e-b167-98d67f8efa43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 726 (d7b7484)\n",
            "main: seed  = 1687558672\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4\n",
            "llama.cpp: loading model from ./models/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 2048\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
            "llama_model_load_internal: using CUDA for GPU acceleration\n",
            "llama_model_load_internal: mem required  = 4951.70 MB (+ 1608.00 MB per state)\n",
            "llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\n",
            "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
            "llama_model_load_internal: offloaded 32/43 layers to GPU\n",
            "llama_model_load_internal: total VRAM used: 10799 MB\n",
            "....................................................................................................\n",
            "llama_init_from_file: kv self size  = 1600.00 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m ### Instruction: Write a story about llamas\\n### Response:\u001b[0m Once upon a time, in the Andes Mountains of South America, there lived a herd of llamas. They were known for their gentle nature and ability to carry heavy loads on their backs. The llamas spent their days grazing in the grassy meadows and resting in the shade of the trees.\n",
            "One day, a group of tourists arrived at the llama's home. They were amazed by how friendly and calm the llamas were. The tourists took pictures with the llamas and even rode on their backs. The llamas seemed to enjoy the attention and posed for photos with big smiles on their faces.\n",
            "As the day went on, the llamas showed off their talents by carrying heavy bags of food and water for the tourists. They made sure to balance the loads carefully so as not to hurt themselves or the humans they were helping.\n",
            "At the end of the day, the llama herd returned to their homes, content with the knowledge that they had brought joy to people from all over the world. From then on, the llamas became known as the friendliest animals in the Andes Mountains, and tourists flocked to see them year after year. [end of text]\n",
            "\n",
            "llama_print_timings:        load time = 85791.90 ms\n",
            "llama_print_timings:      sample time =   185.37 ms /   254 runs   (    0.73 ms per token,  1370.23 tokens per second)\n",
            "llama_print_timings: prompt eval time = 14228.00 ms /    17 tokens (  836.94 ms per token,     1.19 tokens per second)\n",
            "llama_print_timings:        eval time = 126905.51 ms /   253 runs   (  501.60 ms per token,     1.99 tokens per second)\n",
            "llama_print_timings:       total time = 141415.33 ms\n"
          ]
        }
      ]
    }
  ]
}